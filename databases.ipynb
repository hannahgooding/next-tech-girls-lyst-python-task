{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cee47d2c-25b5-48d9-afc3-c98024c5b4a8",
   "metadata": {},
   "source": [
    "# Databases Worksheet\n",
    "\n",
    "This worksheet takes you through migrating the data in our JSON file format, into a 'real' database - what this means, why anyone does this, and what the pros/cons are of different database solutions.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this worksheet, you will:\n",
    "- Understand why databases are needed vs file storage\n",
    "- Know the difference between SQL and NoSQL databases  \n",
    "- Set up PostgreSQL and MongoDB locally\n",
    "- Migrate JSON data into both database types\n",
    "- Write queries in both SQL and MongoDB\n",
    "- Compare performance between different storage methods\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. What is a database, and why use one at all?\n",
    "2. Types of databases\n",
    "3. Migrating to SQL\n",
    "4. Migrating to MongoDB\n",
    "5. Benchmark Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5392edc4-cbd4-4cb0-b8e6-cd85bb21f7f8",
   "metadata": {},
   "source": [
    "## 1. What is a database, and why use one at all?\n",
    "\n",
    "A database is a server that acts as a common place to store data. Ideally, databases are:\n",
    "\n",
    "- easy to access over a network,\n",
    "- scaleable with entries in the database,\n",
    "- able to handle requests concurrently, i.e. at the same time,\n",
    "- predictable\n",
    "\n",
    "At the moment, this project uses your machine's file system as a database store; when you run the python server and open localhost:8000, the server loads `data.jsonl` into the browser. This is fine for a local project, but there are some inherent limitations, like:\n",
    "\n",
    "- if you change the data in `data.jsonl`, you need to restart the server to see it reflected in the client (the browser)\n",
    "- the webpage will perform slower and slower the bigger the dataset is, because each time you load the website the whole list needs to be rendered on the client,\n",
    "- Even if you've implemented some pagination to prevent the previous point, you have a bigger issue of not being able to deploy your app on any other machine. The website currently needs your laptop to show the 'right' data. If you change that data, it only changes for you locally on your machine! So this solution is not scaleable.\n",
    "- You can write code functions to filter your data in `data.jsonl`, but there's no real way to 'query' it. As you probably noticed during the filtering task of your assignment, if you want to find red and blue dresses, you need to write a whole new python function for that to work - or at least modify your existing functions - and then probably loop through your dataset. This is slow and inefficient, especially with larger datasets. Databases provide a query language, by which you can efficiently and quickly retrieve records (entries in the database) based on a flexible criteria.\n",
    "\n",
    "For these reasons, software products almost always come with non-locally hosted databases that serve their content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cfbf4a-867b-4b9f-9c9f-2b331d2c7b60",
   "metadata": {},
   "source": [
    "## 2. Types of databases\n",
    "\n",
    "There are two types of databases; relational, and non-relational.\n",
    "\n",
    "**Relational** databases have data that is structured into tables. All relational database servers (RDS) use SQL, or Structured Query Language, to allow clients to access entries in the database. SQL isn't _really_ a programming language, but a framework to query data. The 'flavor' of SQL you'll use will differ depending on which RDS you go for. Widely used free open-source options include [MySQL](https://www.mysql.com/), [PostgreSQL](https://www.postgresql.org/) and [MariaDB](https://mariadb.org/). There are commercial options around like Oracle, Microsoft's SQL server and ones associated with any cloud network like AWS or Azure.\n",
    "\n",
    "**Non-relational** databases have non structured data. These can contain anything from graphs (such as [neo4j](https://neo4j.com/docs/)), to documents (such as [mongodb](https://www.mongodb.com/docs/)), to key-value pairs (like [redis](https://redis.io/docs/latest/)).\n",
    "\n",
    "Which type you go for, and exactly which server you choose (i.e. MySQL vs Postgresql) will depend on:\n",
    "\n",
    "- the kind of data you need to surface,\n",
    "- how you want to query that data, i.e. if you need to store customer details and client details, you might want to use SQL if you think you'll need to ever join them together (i.e. you want to find out which of your customers have done business with which clients),\n",
    "- personal preference on the specific RDS, for example the type of SQL you write with PostgreSQL is different to what you'd use for MariaDB... while it's largely similar-looking, the exact functions you have access to will differ between servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccf655a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Type Comparison:\n",
      "====================================================================================================\n",
      "Data Structure       | Tables with rows/columns            | Documents (JSON-like)\n",
      "Query Language       | SQL                                 | Native queries\n",
      "Scalability          | Vertical scaling                    | Horizontal scaling\n",
      "Schema               | Fixed schema                        | Flexible schema\n",
      "Use Case             | Complex relationships, transactions | Rapid development, unstructured data\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Visual comparison of database types\n",
    "import pandas as pd\n",
    "\n",
    "# Create a comparison table\n",
    "comparison_data = {\n",
    "    'Aspect': [\n",
    "        'Data Structure',\n",
    "        'Query Language', \n",
    "        'Scalability',\n",
    "        'Schema',\n",
    "        'Use Case'\n",
    "    ],\n",
    "    'Relational (SQL)': [\n",
    "        'Tables with rows/columns',\n",
    "        'SQL',\n",
    "        'Vertical scaling',\n",
    "        'Fixed schema',\n",
    "        'Complex relationships, transactions'\n",
    "    ],\n",
    "    'Document (NoSQL)': [\n",
    "        'Documents (JSON-like)',\n",
    "        'Native queries',\n",
    "        'Horizontal scaling', \n",
    "        'Flexible schema',\n",
    "        'Rapid development, unstructured data'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"Database Type Comparison:\")\n",
    "print(\"=\" * 100)\n",
    "for _, row in df.iterrows():\n",
    "    print(f\"{row['Aspect']:20} | {row['Relational (SQL)']:35} | {row['Document (NoSQL)']}\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cb8126-d29a-4e06-960d-734bbe59bb6c",
   "metadata": {},
   "source": [
    "## 3. Using a relational database: PostgreSQL\n",
    "\n",
    "We are going to migrate the data currently in `data.jsonl` and put it in a relational database first.\n",
    "\n",
    "Ironically, we're actually going to set up a local version of a PostgreSQL server on your machine - the very problem we're trying to get away from! But the key thing to note here is, it doesn't _have_ to be hosted on your machine - any of the database servers mentioned in this document can be hosted on a remote machine, i.e. a remote server, and you can usually authenticate to it and query it in just the same way as we're going to demonstrate both for this task (3.) and the next (4.). There is not such an option for file system storage; while you _could_ connect to a remote machine via SSH for example, and access its files, querying them is a lot more inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e7f198-2a2d-49ee-8802-07444b66cb82",
   "metadata": {},
   "source": [
    "### 3.1 Setting up a PostgreSQL server locally\n",
    "\n",
    "#### 3.1.1 Basic installation and setup\n",
    "We can install postgres on your laptop by following the [official documentation](https://www.postgresql.org/download/macosx/)... A lot of the commands here are also taken from the relevant section of [this guide](https://www.tigerdata.com/learn/how-to-install-postgresql-on-macos).\n",
    "\n",
    "```bash\n",
    "brew install postgresql\n",
    "```\n",
    "\n",
    "This will take a couple of minutes to run. After this, you can start the server in the background.\n",
    "\n",
    "```bash\n",
    "brew services start postgresql\n",
    "```\n",
    "\n",
    "... and verify it's running with,\n",
    "\n",
    "```bash\n",
    "brew services info postgresql\n",
    "```\n",
    "\n",
    "which for me shows something similar to:\n",
    "\n",
    "```bash\n",
    "postgresql@14 (homebrew.mxcl.postgresql@14)\n",
    "Running: ✔\n",
    "Loaded: ✔\n",
    "Schedulable: ✘\n",
    "User: rosesyrett\n",
    "PID: 75325\n",
    "```\n",
    "\n",
    "Every time you log into your machine, it should spin up a local postgres server for you. You can verify the location of the postgres data storage by connecting to the server using\n",
    "\n",
    "```bash\n",
    "psql postgres\n",
    "```\n",
    "\n",
    "and then typing this command,\n",
    "```postgres\n",
    "postgres=# SHOW data_directory;\n",
    "         data_directory\n",
    "---------------------------------\n",
    " /opt/homebrew/var/postgresql@14\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f609e0f-906e-4ea3-b00f-f933c8c36951",
   "metadata": {},
   "source": [
    "#### 3.1.2 Moving data into your locally hosted DB\n",
    "\n",
    "From the previous section, you _could_ in theory make a bunch of 'INSERT' statements to insert a record into a new table that you create in a postgres server terminal.\n",
    "\n",
    "However, because we have 10,000 records to enter, you might spend a long time doing this! So a more efficient way is to download a python client library to connect to your database instead, and programatically read the contents of `data.jsonl` into the database. We only need to do this once - we can rest assured that the data location of the local postgres server isn't going to get wiped. The most popular client library for python is [psycopg](https://www.psycopg.org/docs/) which we will use now.\n",
    "\n",
    "##### 3.1.2.1 Add psycopg into requirements.txt\n",
    "\n",
    "The easiest way to install any new package into your project, is find it in pypi, the online python package repository. [Here is the page for psycopg2](https://pypi.org/project/psycopg2/).\n",
    "\n",
    "From here, navigate to the 'releases' and find the latest stable release. We can see [from the prerequisites](https://www.psycopg.org/docs/install.html#prerequisites) that psycopg2 supports postgresql versions 7.4 to 18, and in the previous section we've spotted that it installed version 14 for me - so we know we're good to go with the latest stable version, which at the time of writing is 2.9.11.\n",
    "\n",
    "Therefore, just add this line into `requirements.txt`:\n",
    "\n",
    "```\n",
    "psycopg2>=2.9.11\n",
    "```\n",
    "\n",
    "##### 3.1.2.2 Install requirements.txt into your virtual environment\n",
    "\n",
    "Active your virtual environment...\n",
    "\n",
    "```bash\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "... and install the requirements into it\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "##### 3.1.2.3 Connect to your db from python\n",
    "\n",
    "You can connect to a postgres instance by using `psycopg2.connect`, passing the database name, user, password and port. Some of these have default values we don't need to supply, such as the port (which is always 5432 when running locally), but in general `brew services start postgresql` will spin up a postgres database with:\n",
    "\n",
    "- database name of \"postgres\" - you can see this as the text before where you type commands in the server terminal, see the code block at the bottom of section 3.1.1\n",
    "- username which is the same as your current user, but you can also find this inside the server terminal by typing the command `\\du`.\n",
    "- an empty password.\n",
    "\n",
    "Therefore, for me to connect to my instance I need to run the following..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab87c2dd-9e22-48dd-8426-a0294e99b7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL successfully!\n",
      "Server version: 140020\n"
     ]
    }
   ],
   "source": [
    "import psycopg\n",
    "\n",
    "# Test connection to verify setup\n",
    "with psycopg.connect(dbname=\"postgres\", user=\"rosesyrett\", password=\"\") as conn:\n",
    "    print(\"Connected to PostgreSQL successfully!\")\n",
    "    print(f\"Server version: {conn.info.server_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b20e4-5150-4020-a64e-27e5e4c9da5f",
   "metadata": {},
   "source": [
    "##### 3.1.2.4 Run a batch of insert statements to copy data to the DB\n",
    "\n",
    "To insert data into our database, we're going to need to create a database table. That means we need to know all the fields we want to insert, as well as their datatypes.\n",
    "\n",
    "PostgreSQL supports several different datatypes, which are [documented here](https://www.postgresql.org/docs/current/datatype.html#DATATYPE-TABLE).\n",
    "\n",
    "A single product contains the following fields:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"product_id\": 1055705468,\n",
    "    \"color\": \"blue\",\n",
    "    \"gender\": \"M\",\n",
    "    \"product_type\": \"clothing\",\n",
    "    \"category\": \"jackets\",\n",
    "    \"subcategory\": \"waistcoats\",\n",
    "    \"designer\": \"dolce-gabbana\",\n",
    "    \"retailer\": \"thecorner\",\n",
    "    \"on_sale\": False,\n",
    "    \"regular_price\": 1198.00,\n",
    "    \"discount_price\": 1198.00,\n",
    "    \"short_description\": \"...\",\n",
    "    \"long_description\": \"...\",\n",
    "    \"image_url\": \"...\",\n",
    "    \"combined_score\": 2.74026,\n",
    "    \"popularity_score\": 0.85,\n",
    "    \"conversion_score\": 1.51859\n",
    "}\n",
    "```\n",
    "\n",
    "So to convert them into something PostgreSQL can understand we should think about the datatypes for each field.\n",
    "\n",
    "An `integer` type has the description, 'signed eight-byte integer'. What does this mean, and can we store our product_id with this type?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb1470b-1a43-4ed6-9c77-2efab824e893",
   "metadata": {},
   "source": [
    "###### Aside: bits and bytes\n",
    "\n",
    "1 byte contains 8 bits, and computers only have so much memory they allocate for numbers. Floats are similar; there's only so many decimal places you can store a float as. You might have come across this in python, especially numpy, with types like `np.int32` or `np.int64` - this means 32 bit and 64 bit integers respectively.\n",
    "\n",
    "An unsigned int means a positive number. A signed int means the sign (i.e. + or -) is stored as part of the memory representation of that number, so it can be positive or negative. Either way, there are maximal values that these numbers can physically be in memory, because of how many bits they take up.\n",
    "\n",
    "In general, to calculate how big your base 10 number (e.g. 1055705468) can get based on the number of bits you can store it in, use this formula...\n",
    "\n",
    "$\n",
    "2^{N} - 1\n",
    "$\n",
    "\n",
    "... where $N$ is the number of bits. So an unsigned `np.int32` can store any number from 0 to $2^{32} - 1 = 4294967295$, and an unsigned integer can store half of this (half of it with a minus sign, half with a plus).\n",
    "\n",
    "Therefore, is the datatype 'integer' enough for our use case? Each integer is 4 bytes, and its signed, so that means we can have \n",
    "\n",
    "$\n",
    "\\frac{2^{8*4} - 1}{2} = 2147483648\n",
    "$\n",
    "\n",
    "Let's find programatically if we can use 4 byte or 8 byte integers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec7ad02f-d54a-49d5-9de5-35bb9c3cf598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 8 byte integer enough space for the product IDs?  True\n",
      "Is 4 byte integer enough space for the product IDs?  False\n"
     ]
    }
   ],
   "source": [
    "from filter import load_products\n",
    "\n",
    "products = load_products()\n",
    "\n",
    "# check all product_ids are unique...\n",
    "\n",
    "seen_ids = {product[\"product_id\"] for product in products}\n",
    "\n",
    "assert len(seen_ids) == len(products), \"product IDs are not unique!\"\n",
    "\n",
    "print(\"Is 8 byte integer enough space for the product IDs? \", max(seen_ids) < 2**(8*4) -1 )\n",
    "print(\"Is 4 byte integer enough space for the product IDs? \", max(seen_ids) < 2**(4*4) -1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8578d337-a514-49d7-bc15-0cfdc41be864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table already exists, skipping creation and data insertion.\n",
      "Sample record from the products table: (1179406705, 'blue', 'M', 'clothing', 'jackets', 'formal jackets', 'dolce-gabbana', 'seymayka', True, 3733.0, 1450.7, 'Navy Blue Double Breasted Suit Coat Blazer', 'DOLCE & GABBANAGorgeous brand new, 100% Authentic DOLCE & GABBANA suit has two button and three pockets. Model: Formal blazer suitStyle: Double breasted two buttonFitting: RegularColor: Navy BlueMaterial: 88% Flax 11% Virgin Wool 1% ElastaneLining: 70% Cotton 30% SilkThree outside pockets and two inside pocketsOne open vent in the backLogo detailsMade in ItalyVery exclusive and high craftsmanship.', 'https://cdna.lystit.com/300/379/tr/photos/seymayka/c725f533/dolce-gabbana--Navy-Blue-Double-Breasted-Suit-Coat-Blazer.jpeg', 3.18095, 3.45619, 2.38633)\n",
      "Total records in database: 10000\n"
     ]
    }
   ],
   "source": [
    "import psycopg\n",
    "from filter import load_products\n",
    "\n",
    "products = load_products()\n",
    "\n",
    "TABLE_NAME = \"products\"\n",
    "\n",
    "# Connect to an existing database\n",
    "with psycopg.connect(dbname=\"postgres\", user=\"rosesyrett\", password=\"\") as conn:\n",
    "    # Open a cursor to perform database operations\n",
    "    with conn.cursor() as cur:\n",
    "        # First, check if the 'products' table already exists! This will be true if you re-run this cell...\n",
    "        cur.execute(\"select exists(select * from information_schema.tables where table_name=%s)\", (TABLE_NAME,))\n",
    "        table_exists = cur.fetchone()[0]\n",
    "\n",
    "        if not table_exists:\n",
    "            print(\"table does not exist! Creating table: \", TABLE_NAME)\n",
    "            # Execute a command: this creates a new 'products' table if it doesn't already exist!\n",
    "            cur.execute(\"\"\"\n",
    "                CREATE TABLE {} (\n",
    "                    product_id bigint PRIMARY KEY,\n",
    "                    color text,\n",
    "                    gender text,\n",
    "                    product_type text,\n",
    "                    category text,\n",
    "                    subcategory text,\n",
    "                    designer text,\n",
    "                    retailer text,\n",
    "                    on_sale boolean,\n",
    "                    regular_price float,\n",
    "                    discount_price float,\n",
    "                    short_description text,\n",
    "                    long_description text,\n",
    "                    image_url text,\n",
    "                    combined_score float,\n",
    "                    popularity_score float,\n",
    "                    conversion_score float\n",
    "                )\n",
    "                \"\"\".format(TABLE_NAME))\n",
    "    \n",
    "            # Pass data to fill a query placeholders and let Psycopg perform\n",
    "            # the correct conversion (no SQL injections!)\n",
    "            cur.executemany(\n",
    "                \"\"\"INSERT INTO {} (\n",
    "                    product_id,\n",
    "                    color,\n",
    "                    gender,\n",
    "                    product_type,\n",
    "                    category,\n",
    "                    subcategory,\n",
    "                    designer, \n",
    "                    retailer, \n",
    "                    on_sale, \n",
    "                    regular_price, \n",
    "                    discount_price, \n",
    "                    short_description, \n",
    "                    long_description, \n",
    "                    image_url,\n",
    "                    combined_score,\n",
    "                    popularity_score,\n",
    "                    conversion_score\n",
    "                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\"\".format(TABLE_NAME),\n",
    "                [tuple(product.values()) for product in products]\n",
    "            )\n",
    "            print(f\"Inserted {len(products)} products into the database!\")\n",
    "        else:\n",
    "            print(\"Table already exists, skipping creation and data insertion.\")\n",
    "\n",
    "        # Query the database and obtain data as Python objects.\n",
    "        cur.execute(f\"SELECT * FROM {TABLE_NAME} LIMIT 1;\")\n",
    "        sample_record = cur.fetchone()\n",
    "        print(f\"Sample record from the {TABLE_NAME} table: {sample_record}\")\n",
    "\n",
    "        # Get total count\n",
    "        cur.execute(f\"SELECT COUNT(*) FROM {TABLE_NAME};\")\n",
    "        total_count = cur.fetchone()[0]\n",
    "        print(f\"Total records in database: {total_count}\")\n",
    "\n",
    "        # Make the changes to the database persistent\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36204b38-b38b-4893-80db-5a3e51b02d50",
   "metadata": {},
   "source": [
    "### 3.2 Querying the DB\n",
    "\n",
    "There are many ways to query the database. You could go directly through a postgresql terminal, like this...\n",
    "\n",
    "```sql\n",
    "postgres=# SELECT count(*) FROM products WHERE color = 'black';\n",
    " count\n",
    "-------\n",
    "  2998\n",
    "(1 row)\n",
    "```\n",
    "\n",
    "You could also use the python library similarly to what we did above..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "056d78a6-0b66-44e2-abde-b4a8efe6f389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of black products:  2998\n",
      "First black product looks like:  (1151539922, 'black', 'F', 'shoes', 'flats', 'flat sandals', 'tj-maxx', 'tjmaxx', True, 99.99, 67.0, 'Clementine Slingback Flats For Leather', 'Lightly Padded Footbed, Low Block Heel, Glitter Finish, Textured Design, Leather Lining, Jewel Embellished Hardware Detail, Size Chart Conversion May Vary Slightly By Country, Goring Detail For Stretch, Pointy Toe, Man Made Sole, Slingback Closure, Textile Upper, Imported, Made In Spain | Clementine Slingback Flats For Women, Leather', 'https://cdna.lystit.com/300/379/tr/photos/tjmaxx/55d968b7/tj-maxx-Sparkle-Black-Clementine-Slingback-Flats-For-Leather.jpeg', 1.4149, 1.8213, 4.61333)\n"
     ]
    }
   ],
   "source": [
    "import psycopg\n",
    "\n",
    "with psycopg.connect(dbname=\"postgres\", user=\"rosesyrett\", password=\"\") as conn:\n",
    "    # Open a cursor to perform database operations\n",
    "    with conn.cursor() as cur:\n",
    "        # Query the database and obtain data as Python objects.\n",
    "        cur.execute(\"SELECT * FROM products WHERE color = 'black';\")\n",
    "        black_products = cur.fetchall()\n",
    "\n",
    "print(\"Total number of black products: \", len(black_products))\n",
    "if black_products:\n",
    "    print(\"First black product looks like: \", black_products[0])\n",
    "else:\n",
    "    print(\"No black products found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edd13a8-d6c5-4e22-aff8-35c0d8d541af",
   "metadata": {},
   "source": [
    "Hopefully you can see how we could replace some of the filtering on our website with calls to the database!\n",
    "\n",
    "**As an exercise**, make a new branch of your repository called `postgres` and replace all the places in your code where you would normally read from `data.jsonl` with something that will fetch products, similarly to what we've done in the code snippet above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179e9112-f083-42ab-94b1-75cceb32caf2",
   "metadata": {},
   "source": [
    "## 4. Using a non-relational database: MongoDB\n",
    "\n",
    "You've just played around with a relational database, which are great when you have multiple tables that relate to each other. In our case, we have a simple database with no relations yet - so, we can decide to use a non-relational database instead, like mongodb.\n",
    "\n",
    "For this section, it is recommended you check out your main branch and make a new branch, called 'mongodb' or similar, to do this task in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e04a01f-5422-4eaf-912d-427a36d801c2",
   "metadata": {},
   "source": [
    "### 4.1 Setting up MongoDB locally\n",
    "\n",
    "#### 4.1.1 Basic installation and setup\n",
    "\n",
    "Mongodb publishes some good [documentation](https://www.mongodb.com/docs/manual/administration/install-community/?operating-system=macos&macos-installation-method=homebrew) on how to do this, but the commands you need to use are summarised here:\n",
    "\n",
    "```bash\n",
    "$ xcode-select --install\n",
    "$ brew tap mongodb/brew\n",
    "$ brew update\n",
    "$ brew install mongodb-community@8.2\n",
    "```\n",
    "\n",
    "... Then, to start mongodb locally use this command...\n",
    "```bash\n",
    "$ brew services start mongodb-community@8.2\n",
    "```\n",
    "\n",
    "... now, if you list your services you'll see both mongodb and postgres running locally:\n",
    "```bash\n",
    "$ brew services list\n",
    "Name              Status  User       File\n",
    "mongodb-community started rosesyrett ~/Library/LaunchAgents/homebrew.mxcl.mongodb-community.plist\n",
    "postgresql@14     started rosesyrett ~/Library/LaunchAgents/homebrew.mxcl.postgresql@14.plist\n",
    "```\n",
    "\n",
    "Finally, just like with postgres, mongodb comes equipped with a handy CLI tool that lets us connect to instances of these databases. For mongo, we can use this command:\n",
    "\n",
    "```bash\n",
    "$ mongosh\n",
    "Current Mongosh Log ID:\t699330fdf60262079406e71c\n",
    "Connecting to:\t\tmongodb://127.0.0.1:27017/?directConnection=true&serverSelectionTimeoutMS=2000&appName=mongosh+2.7.0\n",
    "Using MongoDB:\t\t8.2.5\n",
    "Using Mongosh:\t\t2.7.0\n",
    "\n",
    "For mongosh info see: https://www.mongodb.com/docs/mongodb-shell/\n",
    "test> \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce2588a-107e-4147-ae79-5c47cb846f9d",
   "metadata": {},
   "source": [
    "#### 4.1.2 Moving data into your locally hosted DB\n",
    "\n",
    "Mongodb provides a python client, to connect to mongo databases, which is [documented here](http://mongodb.com/docs/languages/python/pymongo-driver/current/connect/mongoclient/).\n",
    "\n",
    "##### 4.1.2.1 Add pymongo into requirements.txt\n",
    "\n",
    "The mongo documentation references pymongo, which is documented in pypi [here](https://pypi.org/project/pymongo/).\n",
    "\n",
    "The latest stable release at the time of writing is 4.16.0, so we can just add this line into `requirements.txt`:\n",
    "\n",
    "```\n",
    "pymongo>=4.16.0\n",
    "```\n",
    "\n",
    "##### 4.1.2.2 Install requirements.txt into your virtual environment\n",
    "\n",
    "Active your virtual environment...\n",
    "\n",
    "```bash\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "... and install the requirements into it\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "##### 4.1.2.3 Connect to your db from python\n",
    "\n",
    "To connect to the db, we will need the URI of our mongodb. Notice in the previous section, when you ran `mongosh` it gave you this information - which means you should just be able to connect with this! By default, your local database does not have a username and password, but for a 'real' database (probably hosted on another server, i.e. not just your local machine) you would need to provide these details when creating the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9103401c-8fef-4051-856f-bcff70cb3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "uri = \"mongodb://localhost:27017/\" # note: The IP address 127.0.0.1 is the same as localhost! It just means it runs on your machine only.\n",
    "client = MongoClient(uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5ec8e0-187e-428a-8d44-3f3cdfb4a6af",
   "metadata": {},
   "source": [
    "##### 4.1.2.4 Copy data to the DB\n",
    "\n",
    "MongoDB stores JSON blobs in collections, and collections in databases. Right now, if you go to a mongo shell, you can see you have no collections and a few small databases:\n",
    "\n",
    "```mongodb\n",
    "test> show collections\n",
    "\n",
    "test> show databases\n",
    "admin   40.00 KiB\n",
    "config  60.00 KiB\n",
    "local   40.00 KiB\n",
    "```\n",
    "\n",
    "A collection is similar to a database table, like the 'products' table we made in the previous section. We can go ahead and create a new database and a new collection for our products to live..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1db6323c-3362-4f70-8b82-e6d206e8f700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'test_database'), 'products') has 10000 documents!\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from filter import load_products\n",
    "\n",
    "products = load_products()\n",
    "\n",
    "uri = \"mongodb://localhost:27017/\" # note: The IP address 127.0.0.1 is the same as localhost! It just means it runs on your machine only.\n",
    "client = MongoClient(uri)\n",
    "\n",
    "# create a database\n",
    "database = client[\"test_database\"]\n",
    "\n",
    "# create a collection inside that database to store our products\n",
    "collection = database[\"products\"]\n",
    "\n",
    "if collection.count_documents({}) == 0:\n",
    "    result = collection.insert_many(products)\n",
    "\n",
    "count = collection.count_documents({})\n",
    "print(collection, \"has\", count, \"documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12428a6-55ed-4648-821e-0444dede3c3b",
   "metadata": {},
   "source": [
    "### 4.2 Querying the DB\n",
    "\n",
    "Just like with postgres, there are multiple ways to query the database. If you wanted to use the mongo shell, you could simply enter it like so...\n",
    "```bash\n",
    "$ mongosh\n",
    "```\n",
    "\n",
    "... and then go into your new database, and filter objects in the collection\n",
    "```mongosh\n",
    ">test use test_database\n",
    "switched to db test_database\n",
    "test_database> show collections\n",
    "products\n",
    "test_database> db.products.find({})\n",
    "[\n",
    "    ...\n",
    "    {\n",
    "        _id: ObjectId('699335ae4db0be355d8e5cd1'),\n",
    "        product_id: 1087619417,\n",
    "        color: 'gray',\n",
    "        gender: 'M',\n",
    "        product_type: 'clothing',\n",
    "        category: 'coats',\n",
    "        subcategory: 'trench coats',\n",
    "        designer: 'burberry',\n",
    "        retailer: 'senser',\n",
    "        on_sale: true,\n",
    "        regular_price: 1802.5,\n",
    "        discount_price: 1687.5,\n",
    "        short_description: 'Double-Breasted Long Trench Coat',\n",
    "        long_description: 'Double-breasted long trench coat, long sleeves, lapel, waist tie details.',\n",
    "        image_url: 'https://cdna.lystit.com/300/379/tr/photos/senser/d2643d6e/burberry--Double-Breasted-Long-Trench-Coat.jpeg',\n",
    "        combined_score: 2.83343,\n",
    "        popularity_score: 2.1646,\n",
    "        conversion_score: 1.55012\n",
    "    }\n",
    "]\n",
    "Type \"it\" for more\n",
    "test_database> db.products.find({ color: \"black\"})\n",
    "...\n",
    "test_database> db.products.countDocuments({ color: \"black\"})\n",
    "2998\n",
    "```\n",
    "\n",
    "You could also use the python library similarly to what we did above..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52332706-9a5c-4cf6-bfd1-475f27868977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pymongo.synchronous.cursor.Cursor object at 0x11c82bad0>\n",
      "{'_id': ObjectId('699335ae4db0be355d8e5cc1'), 'product_id': 1151539922, 'color': 'black', 'gender': 'F', 'product_type': 'shoes', 'category': 'flats', 'subcategory': 'flat sandals', 'designer': 'tj-maxx', 'retailer': 'tjmaxx', 'on_sale': True, 'regular_price': 99.99, 'discount_price': 67.0, 'short_description': 'Clementine Slingback Flats For Leather', 'long_description': 'Lightly Padded Footbed, Low Block Heel, Glitter Finish, Textured Design, Leather Lining, Jewel Embellished Hardware Detail, Size Chart Conversion May Vary Slightly By Country, Goring Detail For Stretch, Pointy Toe, Man Made Sole, Slingback Closure, Textile Upper, Imported, Made In Spain | Clementine Slingback Flats For Women, Leather', 'image_url': 'https://cdna.lystit.com/300/379/tr/photos/tjmaxx/55d968b7/tj-maxx-Sparkle-Black-Clementine-Slingback-Flats-For-Leather.jpeg', 'combined_score': 1.4149, 'popularity_score': 1.8213, 'conversion_score': 4.61333}\n",
      "{'_id': ObjectId('699335ae4db0be355d8e5cc2'), 'product_id': 1163522043, 'color': 'black', 'gender': 'M', 'product_type': 'clothing', 'category': 'shorts', 'subcategory': 'casual shorts', 'designer': 'rick-owens', 'retailer': 'revolveclothing', 'on_sale': False, 'regular_price': 929.76864975, 'discount_price': 929.76864975, 'short_description': 'Pusher Shorts', 'long_description': 'DRKSHDW by Rick Owens Pusher Shorts in Black. Size L. Also in M, S. DRKSHDW by Rick Owens Pusher Shorts in Black. Size M, S. 100% cotton. Hand wash. Drawstring closure. Shorts measure approx 21 in length. DRKS-MF33. DU02E7373-FSL-09.', 'image_url': 'https://cdna.lystit.com/300/379/tr/photos/revolveclothing/7e6d4bd2/rick-owens-Black-Pusher-Shorts.jpeg', 'combined_score': 2.8525, 'popularity_score': 2.2138, 'conversion_score': 2.12039}\n",
      "{'_id': ObjectId('699335ae4db0be355d8e5cc5'), 'product_id': 1156424464, 'color': 'black', 'gender': 'M', 'product_type': 'accessories', 'category': 'sunglasses', 'subcategory': None, 'designer': 'saint-laurent', 'retailer': 'balardi', 'on_sale': True, 'regular_price': 484.445868, 'discount_price': 310.263084, 'short_description': 'Squared Sunglasses Sl 795 Joe 003', 'long_description': 'Saint Laurent\\nSUNGLASSES\\nSQUARED\\nSILVER-SILVER-GREY', 'image_url': 'https://cdna.lystit.com/300/379/tr/photos/balardi/8a79d960/saint-laurent-SILVER-Squared-Sunglasses-Sl-795-Joe-003.jpeg', 'combined_score': 2.95476, 'popularity_score': 3.0642, 'conversion_score': 1.20975}\n"
     ]
    }
   ],
   "source": [
    "black_products = collection.find({ \"color\": \"black\" })\n",
    "\n",
    "print(black_products)\n",
    "\n",
    "for i in range(3):\n",
    "    print(next(black_products))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427e35a3-8474-44a6-a687-c8918c6b10c0",
   "metadata": {},
   "source": [
    "Notice how the return of `collection.find` isn't just all the objects, but an instance of `pymongo.synchronous.cursor.Cursor` instead... so to do something with all the black products, you'd have to iterate through them like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc7445c3-13ad-4229-be98-48d21c04088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for product in black_products:\n",
    "    ... # do something here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2871cd-e4a0-455d-a5fb-e4c64676e212",
   "metadata": {},
   "source": [
    "**As an exercise**, in your `mongodb` branch that you created at the start of this section, replace all the places in your code where you would normally read from `data.jsonl` with something that will fetch products, similarly to what we've done in the code snippet above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d406a555-c89e-4431-83a2-e7339d1873d8",
   "metadata": {},
   "source": [
    "## 5. Benchmark Testing\n",
    "\n",
    "You have now migrated your data from `data.jsonl` into two databases, and done this in two separate branches. Now, it's time to do some testing, to see how much longer or slower the different approaches of data storage and retrieval that we have are!\n",
    "\n",
    "Just by navigating on the website on the main branch, i.e. not having migrated any of the data to any database, we make 'database' calls whenever we need to access the data in `data.jsonl`, especially when we apply filters.\n",
    "\n",
    "So... let's do some benchmark testing!\n",
    "\n",
    "**As an exercise**, switch between the branches `mongodb` and `postgres` and check how long you think it takes to apply a filter on the website. You should be able to do this using google chrome's inspector tools, but you can also add some `time.time()` statements in the codebase and some `print` statements, where you should see your server print out how long it takes.\n",
    "\n",
    "We want to answer the following questions:\n",
    "\n",
    "- with the dataset of products that we have, which option is the fastest? (i.e. file system loading vs postgres vs mongodb)\n",
    "- How consistent is this? Can you write a test file like `test_timings.py` in both of your branches which will tell you how long a query takes over N amount of iterations?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
